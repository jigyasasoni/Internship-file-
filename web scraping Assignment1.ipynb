{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c36dcf",
   "metadata": {},
   "source": [
    "# Web Scraping Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dba409a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from URL: https://example.com\n",
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_data_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Process the data or perform other actions with the response\n",
    "        print(\"Data from URL:\", url)\n",
    "        print(response.text)\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from URL. Status code:\", response.status_code)\n",
    "\n",
    "url_to_scrape = \"https://example.com\"\n",
    "fetch_data_from_url(url_to_scrape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f4520e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Header\n",
      "0                       Main Page\n",
      "1            Welcome to Wikipedia\n",
      "2   From today's featured article\n",
      "3                Did you knowÂ ...\n",
      "4                     In the news\n",
      "5                     On this day\n",
      "6      From today's featured list\n",
      "7        Today's featured picture\n",
      "8        Other areas of Wikipedia\n",
      "9     Wikipedia's sister projects\n",
      "10            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "# 1) Write a python program to display all the header tags from wikipedia.org and make data frame.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "header_tags = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "header_texts = [tag.get_text() for tag in header_tags]\n",
    "\n",
    "df = pd.DataFrame(header_texts, columns=[\"Header\"])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3effaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the web page. Status code: 404\n"
     ]
    }
   ],
   "source": [
    "# Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find('table')\n",
    "\n",
    "names = []\n",
    "terms = []\n",
    "\n",
    "\n",
    "for row in table.find_all('tr')[1:]: \n",
    "columns = row.find_all('td')\n",
    "name = columns[0].text.strip()\n",
    "term = columns[1].text.strip()\n",
    "names.append(name)\n",
    "terms.append(term)\n",
    "\n",
    "\n",
    "data = {\n",
    "'Name': names,\n",
    "'Term of Office': terms\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cbeef6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "               Team Matches Points Rating\n",
      "0        India\\nIND      49  5,839    119\n",
      "1    Australia\\nAUS      36  4,015    112\n",
      "2     Pakistan\\nPAK      32  3,525    110\n",
      "3  South Africa\\nSA      29  3,166    109\n",
      "4   New Zealand\\nNZ      38  4,007    105\n",
      "5      England\\nENG      34  3,377     99\n",
      "6     Sri Lanka\\nSL      43  3,943     92\n",
      "7   Bangladesh\\nBAN      40  3,574     89\n",
      "8  Afghanistan\\nAFG      26  2,170     83\n",
      "9   West Indies\\nWI      38  2,582     68\n",
      "\n",
      "Top 10 ODI Batsmen:\n",
      "                 Batsman Team Rating\n",
      "0             Babar Azam  PAK    829\n",
      "1           Shubman Gill  IND    823\n",
      "2        Quinton de Kock   SA    769\n",
      "3       Heinrich Klaasen   SA    756\n",
      "4           David Warner  AUS    747\n",
      "5            Virat Kohli  IND    747\n",
      "6           Harry Tector  IRE    729\n",
      "7           Rohit Sharma  IND    725\n",
      "8  Rassie van der Dussen   SA    716\n",
      "9            Imam-ul-Haq  PAK    704\n",
      "\n",
      "Top 10 ODI Bowlers:\n",
      "           Bowler Team Rating\n",
      "0  Josh Hazlewood  AUS    670\n",
      "1  Mohammed Siraj  IND    668\n",
      "2  Keshav Maharaj   SA    656\n",
      "3     Rashid Khan  AFG    654\n",
      "4     Trent Boult   NZ    653\n",
      "5   Mohammad Nabi  AFG    641\n",
      "6      Adam Zampa  AUS    635\n",
      "7      Matt Henry   NZ    634\n",
      "8   Kuldeep Yadav  IND    632\n",
      "9  Shaheen Afridi  PAK    625\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_icc_data(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "def scrape_top_odi_teams():\n",
    "    url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "    soup = scrape_icc_data(url)\n",
    "    \n",
    "    if soup:\n",
    "        teams = []\n",
    "        matches = []\n",
    "        points = []\n",
    "        ratings = []\n",
    "        \n",
    "        table = soup.find('table', {'class': 'table'})\n",
    "        rows = table.find_all('tr')\n",
    "        \n",
    "        for row in rows[1:11]:\n",
    "            columns = row.find_all('td')\n",
    "            team = columns[1].text.strip()\n",
    "            match = columns[2].text.strip()\n",
    "            point = columns[3].text.strip()\n",
    "            rating = columns[4].text.strip()\n",
    "            \n",
    "            teams.append(team)\n",
    "            matches.append(match)\n",
    "            points.append(point)\n",
    "            ratings.append(rating)\n",
    "        \n",
    "        data = {\n",
    "            'Team': teams,\n",
    "            'Matches': matches,\n",
    "            'Points': points,\n",
    "            'Rating': ratings\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "\n",
    "def scrape_top_odi_batsmen():\n",
    "    url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "    soup = scrape_icc_data(url)\n",
    "    \n",
    "    if soup:\n",
    "        players = []\n",
    "        teams = []\n",
    "        ratings = []\n",
    "        \n",
    "        table = soup.find('table', {'class': 'table'})\n",
    "        rows = table.find_all('tr')\n",
    "        \n",
    "        for row in rows[1:11]:\n",
    "            columns = row.find_all('td')\n",
    "            player = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            \n",
    "            players.append(player)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "        \n",
    "        data = {\n",
    "            'Batsman': players,\n",
    "            'Team': teams,\n",
    "            'Rating': ratings\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "\n",
    "def scrape_top_odi_bowlers():\n",
    "    url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "    soup = scrape_icc_data(url)\n",
    "    \n",
    "    if soup:\n",
    "        players = []\n",
    "        teams = []\n",
    "        ratings = []\n",
    "        \n",
    "        table = soup.find('table', {'class': 'table'})\n",
    "        rows = table.find_all('tr')\n",
    "        \n",
    "        for row in rows[1:11]:\n",
    "            columns = row.find_all('td')\n",
    "            player = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            \n",
    "            players.append(player)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "        \n",
    "        data = {\n",
    "            'Bowler': players,\n",
    "            'Team': teams,\n",
    "            'Rating': ratings\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    top_odi_teams = scrape_top_odi_teams()\n",
    "    top_odi_batsmen = scrape_top_odi_batsmen()\n",
    "    top_odi_bowlers = scrape_top_odi_bowlers()\n",
    "    \n",
    "    if top_odi_teams is not None:\n",
    "        print(\"Top 10 ODI Teams:\")\n",
    "        print(top_odi_teams)\n",
    "    \n",
    "    if top_odi_batsmen is not None:\n",
    "        print(\"\\nTop 10 ODI Batsmen:\")\n",
    "        print(top_odi_batsmen)\n",
    "    \n",
    "    if top_odi_bowlers is not None:\n",
    "        print(\"\\nTop 10 ODI Bowlers:\")\n",
    "        print(top_odi_bowlers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1a63dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Women's Teams:\n",
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      19  3,084    162\n",
      "1      England\\nENG      23  2,991    130\n",
      "2  South Africa\\nSA      21  2,446    116\n",
      "3        India\\nIND      18  1,745     97\n",
      "4   New Zealand\\nNZ      21  2,014     96\n",
      "5   West Indies\\nWI      18  1,610     89\n",
      "6     Sri Lanka\\nSL       9    714     79\n",
      "7   Bangladesh\\nBAN      11    816     74\n",
      "8     Thailand\\nTHA      11    753     68\n",
      "9     Pakistan\\nPAK      21  1,435     68\n",
      "\n",
      "Top 10 ODI Women's Batsmen:\n",
      "                Batsman Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    807\n",
      "1           Beth Mooney  AUS    750\n",
      "2   Chamari Athapaththu   SL    736\n",
      "3       Laura Wolvaardt   SA    727\n",
      "4       Smriti Mandhana  IND    708\n",
      "5          Alyssa Healy  AUS    698\n",
      "6          Ellyse Perry  AUS    697\n",
      "7      Harmanpreet Kaur  IND    694\n",
      "8           Meg Lanning  AUS    662\n",
      "9        Marizanne Kapp   SA    642\n",
      "\n",
      "Top 10 ODI Women's All-Rounders:\n",
      "            All-Rounder Team Rating\n",
      "0        Marizanne Kapp   SA    385\n",
      "1      Ashleigh Gardner  AUS    377\n",
      "2  Natalie Sciver-Brunt  ENG    360\n",
      "3       Hayley Matthews   WI    358\n",
      "4           Amelia Kerr   NZ    346\n",
      "5         Deepti Sharma  IND    312\n",
      "6          Ellyse Perry  AUS    282\n",
      "7         Jess Jonassen  AUS    227\n",
      "8         Sophie Devine   NZ    227\n",
      "9              Nida Dar  PAK    224\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_icc_data(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "def scrape_top_odi_womens_teams():\n",
    "    url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "    soup = scrape_icc_data(url)\n",
    "    \n",
    "    if soup:\n",
    "        teams = []\n",
    "        matches = []\n",
    "        points = []\n",
    "        ratings = []\n",
    "        \n",
    "        table = soup.find('table', {'class': 'table'})\n",
    "        rows = table.find_all('tr')\n",
    "        \n",
    "        for row in rows[1:11]:\n",
    "            columns = row.find_all('td')\n",
    "            team = columns[1].text.strip()\n",
    "            match = columns[2].text.strip()\n",
    "            point = columns[3].text.strip()\n",
    "            rating = columns[4].text.strip()\n",
    "            \n",
    "            teams.append(team)\n",
    "            matches.append(match)\n",
    "            points.append(point)\n",
    "            ratings.append(rating)\n",
    "        \n",
    "        data = {\n",
    "            'Team': teams,\n",
    "            'Matches': matches,\n",
    "            'Points': points,\n",
    "            'Rating': ratings\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "\n",
    "def scrape_top_odi_womens_batsmen():\n",
    "    url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "    soup = scrape_icc_data(url)\n",
    "    \n",
    "    if soup:\n",
    "        players = []\n",
    "        teams = []\n",
    "        ratings = []\n",
    "        \n",
    "        table = soup.find('table', {'class': 'table'})\n",
    "        rows = table.find_all('tr')\n",
    "        \n",
    "        for row in rows[1:11]:\n",
    "            columns = row.find_all('td')\n",
    "            player = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            \n",
    "            players.append(player)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "        \n",
    "        data = {\n",
    "            'Batsman': players,\n",
    "            'Team': teams,\n",
    "            'Rating': ratings\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "\n",
    "def scrape_top_odi_womens_allrounders():\n",
    "    url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "    soup = scrape_icc_data(url)\n",
    "    \n",
    "    if soup:\n",
    "        players = []\n",
    "        teams = []\n",
    "        ratings = []\n",
    "        \n",
    "        table = soup.find('table', {'class': 'table'})\n",
    "        rows = table.find_all('tr')\n",
    "        \n",
    "        for row in rows[1:11]:\n",
    "            columns = row.find_all('td')\n",
    "            player = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            \n",
    "            players.append(player)\n",
    "            teams.append(team)\n",
    "            ratings.append(rating)\n",
    "        \n",
    "        data = {\n",
    "            'All-Rounder': players,\n",
    "            'Team': teams,\n",
    "            'Rating': ratings\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    top_odi_womens_teams = scrape_top_odi_womens_teams()\n",
    "    top_odi_womens_batsmen = scrape_top_odi_womens_batsmen()\n",
    "    top_odi_womens_allrounders = scrape_top_odi_womens_allrounders()\n",
    "    \n",
    "    if top_odi_womens_teams is not None:\n",
    "        print(\"Top 10 ODI Women's Teams:\")\n",
    "        print(top_odi_womens_teams)\n",
    "    \n",
    "    if top_odi_womens_batsmen is not None:\n",
    "        print(\"\\nTop 10 ODI Women's Batsmen:\")\n",
    "        print(top_odi_womens_batsmen)\n",
    "    \n",
    "    if top_odi_womens_allrounders is not None:\n",
    "        print(\"\\nTop 10 ODI Women's All-Rounders:\")\n",
    "        print(top_odi_womens_allrounders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cb93977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNBC World News:\n",
      "Empty DataFrame\n",
      "Columns: [Headline, Time, News Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame-\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_cnbc_news(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "def scrape_cnbc_world_news():\n",
    "    url = \"https://www.cnbc.com/world/?region=world\"\n",
    "    soup = scrape_cnbc_news(url)\n",
    "    \n",
    "    if soup:\n",
    "        headlines = []\n",
    "        times = []\n",
    "        news_links = []\n",
    "        \n",
    "        articles = soup.find_all('div', class_='Card')\n",
    "        \n",
    "        for article in articles:\n",
    "            headline = article.find('div', class_='Card-title')\n",
    "            time = article.find('time', class_='Card-time')\n",
    "            link = article.find('a', href=True)\n",
    "            \n",
    "            if headline and time and link:\n",
    "                headlines.append(headline.text.strip())\n",
    "                times.append(time['datetime'])\n",
    "                news_links.append(link['href'])\n",
    "        \n",
    "        data = {\n",
    "            'Headline': headlines,\n",
    "            'Time': times,\n",
    "            'News Link': news_links\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cnbc_world_news_df = scrape_cnbc_world_news()\n",
    "    \n",
    "    if cnbc_world_news_df is not None:\n",
    "        print(\"CNBC World News:\")\n",
    "        print(cnbc_world_news_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d64d2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Downloaded Articles in Artificial Intelligence:\n",
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame-\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_elsevier_data(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "def scrape_most_downloaded_articles():\n",
    "    url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "    soup = scrape_elsevier_data(url)\n",
    "    \n",
    "    if soup:\n",
    "        paper_titles = []\n",
    "        authors_list = []\n",
    "        published_dates = []\n",
    "        paper_urls = []\n",
    "        \n",
    "        articles = soup.find_all('div', class_='article-content')\n",
    "        \n",
    "        for article in articles:\n",
    "            title = article.find('a', class_='anchorText')\n",
    "            authors = article.find('div', class_='authors')\n",
    "            date = article.find('div', class_='coverDate')\n",
    "            url = article.find('a', class_='anchorText', href=True)\n",
    "            \n",
    "            if title and authors and date and url:\n",
    "                paper_titles.append(title.text.strip())\n",
    "                authors_list.append(authors.text.strip())\n",
    "                published_dates.append(date.text.strip())\n",
    "                paper_urls.append(url['href'])\n",
    "        \n",
    "        data = {\n",
    "            'Paper Title': paper_titles,\n",
    "            'Authors': authors_list,\n",
    "            'Published Date': published_dates,\n",
    "            'Paper URL': paper_urls\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    most_downloaded_articles_df = scrape_most_downloaded_articles()\n",
    "    \n",
    "    if most_downloaded_articles_df is not None:\n",
    "        print(\"Most Downloaded Articles in Artificial Intelligence:\")\n",
    "        print(most_downloaded_articles_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7e4adf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dineout Restaurants in Delhi:\n",
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to scrape mentioned details from dineout.co.in and make data frame-\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to scrape data from the dineout.co.in website\n",
    "def scrape_dineout_data(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "# Function to scrape and create a DataFrame for restaurant details\n",
    "def scrape_dineout_restaurants():\n",
    "    url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
    "    soup = scrape_dineout_data(url)\n",
    "    \n",
    "    if soup:\n",
    "        restaurant_names = []\n",
    "        cuisines = []\n",
    "        locations = []\n",
    "        ratings = []\n",
    "        image_urls = []\n",
    "        \n",
    "        restaurants = soup.find_all('div', class_='restnt-info-main')\n",
    "        \n",
    "        for restaurant in restaurants:\n",
    "            name = restaurant.find('a', class_='restnt-name ellipsis')\n",
    "            cuisine = restaurant.find('span', class_='double-line-ellipsis')\n",
    "            location = restaurant.find('span', class_='restnt-loc ellipsis')\n",
    "            rating = restaurant.find('div', class_='restnt-rating')\n",
    "            img = restaurant.find('img', class_='no-img')\n",
    "            \n",
    "            if name and cuisine and location and rating and img:\n",
    "                restaurant_names.append(name.text.strip())\n",
    "                cuisines.append(cuisine.text.strip())\n",
    "                locations.append(location.text.strip())\n",
    "                ratings.append(rating.text.strip())\n",
    "                image_urls.append(img['data-src'])\n",
    "        \n",
    "        data = {\n",
    "            'Restaurant Name': restaurant_names,\n",
    "            'Cuisine': cuisines,\n",
    "            'Location': locations,\n",
    "            'Ratings': ratings,\n",
    "            'Image URL': image_urls\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "\n",
    "# Main program to scrape and display the DataFrame\n",
    "if __name__ == \"__main__\":\n",
    "    dineout_restaurants_df = scrape_dineout_restaurants()\n",
    "    \n",
    "    if dineout_restaurants_df is not None:\n",
    "        print(\"Dineout Restaurants in Delhi:\")\n",
    "        print(dineout_restaurants_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5fa08a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
