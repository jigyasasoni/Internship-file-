{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d714c2a",
   "metadata": {},
   "source": [
    "# Web-Scraping-Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3862ccf",
   "metadata": {},
   "source": [
    "# Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b740028e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: et_xmlfile in c:\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1931dd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Amazon.in.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m html_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmazon.in.html\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m     40\u001b[0m excel_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamazon_products.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m---> 42\u001b[0m \u001b[43mparse_html_and_write_to_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexcel_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m, in \u001b[0;36mparse_html_and_write_to_excel\u001b[1;34m(html_file, excel_file)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_html_and_write_to_excel\u001b[39m(html_file, excel_file):\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhtml_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      6\u001b[0m         html_content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      8\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html_content, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Amazon.in.html'"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_html_and_write_to_excel(html_file, excel_file):\n",
    "    with open(html_file, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    result_info_bars = soup.find_all('div', class_='s-widget-container s-spacing-small s-widget-container-height-small celwidget slot=UPPER template=RESULT_INFO_BAR widgetId=result-info-bar')\n",
    "\n",
    "    workbook = openpyxl.Workbook()\n",
    "    worksheet = workbook.active\n",
    "\n",
    "    worksheet.append(['Product_Name', 'Product_Price', 'Product_Reviews'])\n",
    "\n",
    "    for result_info_bar in result_info_bars:\n",
    "        product_name = \"\"\n",
    "        product_price = \"\"\n",
    "        product_reviews = \"\"\n",
    "        \n",
    "        product_name_tag = result_info_bar.find('span', class_='a-size-medium a-color-base a-text-normal')\n",
    "        if product_name_tag:\n",
    "            product_name = product_name_tag.get_text(strip=True)\n",
    "\n",
    "        product_price_tag = result_info_bar.find('span', class_='a-price-whole')\n",
    "        if product_price_tag:\n",
    "            product_price = product_price_tag.get_text(strip=True)\n",
    "            \n",
    "        product_reviews_tag = result_info_bar.find('span', class_='a-icon-alt')\n",
    "        if product_reviews_tag:\n",
    "            product_reviews = product_reviews_tag.get_text(strip=True)\n",
    "\n",
    "        worksheet.append([product_name, product_price, product_reviews])\n",
    "\n",
    "    workbook.save(excel_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    html_file = 'Amazon.in.html' \n",
    "    excel_file = 'amazon_products.xlsx' \n",
    "\n",
    "    parse_html_and_write_to_excel(html_file, excel_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c0c3b",
   "metadata": {},
   "source": [
    "# In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff2e1b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Operation interrupted by the user.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_pages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOperation interrupted by the user.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[43mnum_pages\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     69\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.amazon.in/s?k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproduct_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     70\u001b[0m     driver\u001b[38;5;241m.\u001b[39mimplicitly_wait(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_pages' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def scrape_product_details(product):\n",
    "    details = {\n",
    "        'Brand Name': '-',\n",
    "        'Name of the Product': '-',\n",
    "        'Price': '-',\n",
    "        'Return/Exchange': '-',\n",
    "        'Expected Delivery': '-',\n",
    "        'Availability': '-',\n",
    "        'Product URL': '-'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        details['Brand Name'] = product.find('span', class_='a-size-base-plus').get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        details['Name of the Product'] = product.find('span', class_='a-text-normal').get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        details['Price'] = product.find('span', class_='a-offscreen').get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        details['Return/Exchange'] = product.find('div', class_='a-row a-size-base a-color-secondary').get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        details['Expected Delivery'] = product.find('span', class_='a-text-bold').get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        details['Availability'] = product.find('span', class_='a-size-medium a-color-success').get_text(strip=True)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        details['Product URL'] = 'https://www.amazon.in' + product.find('a', class_='a-link-normal')['href']\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    return details\n",
    "\n",
    "def search_amazon_and_scrape(product_name, num_pages=3):\n",
    "    driver = webdriver.Chrome(executable_path='/path/to/chromedriver')\n",
    "\n",
    "    product_details_list = []\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    try:\n",
    "        product_name = input('Enter the product you want to search on Amazon.in: ')\n",
    "        search_amazon_and_scrape(product_name)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nOperation interrupted by the user.\")\n",
    "        \n",
    "    for page in range(1, num_pages + 1):\n",
    "        driver.get(f'https://www.amazon.in/s?k={product_name}&page={page}')\n",
    "        driver.implicitly_wait(10)\n",
    "        \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        products = soup.find_all('div', class_='s-result-item')\n",
    "\n",
    "        for product in products:\n",
    "            details = scrape_product_details(product)\n",
    "            product_details_list.append(details)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    df = pd.DataFrame(product_details_list)\n",
    "\n",
    "    df.to_csv('amazon_product_details.csv', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    product_name = input('Enter the product you want to search on Amazon.in: ')\n",
    "    search_amazon_and_scrape(product_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6de73c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google_images_download in c:\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: selenium in c:\\anaconda3\\lib\\site-packages (from google_images_download) (4.15.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\anaconda3\\lib\\site-packages (from selenium->google_images_download) (0.23.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\anaconda3\\lib\\site-packages (from selenium->google_images_download) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\anaconda3\\lib\\site-packages (from selenium->google_images_download) (2022.12.7)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\anaconda3\\lib\\site-packages (from selenium->google_images_download) (1.26.14)\n",
      "Requirement already satisfied: outcome in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->google_images_download) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->google_images_download) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->google_images_download) (1.1.3)\n",
      "Requirement already satisfied: idna in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->google_images_download) (3.4)\n",
      "Requirement already satisfied: sortedcontainers in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->google_images_download) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->google_images_download) (1.15.1)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium->google_images_download) (22.1.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium->google_images_download) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium->google_images_download) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium->google_images_download) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->google_images_download) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install google_images_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdcefac",
   "metadata": {},
   "source": [
    "# Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "912b7690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\anaconda3\\lib\\site-packages (4.15.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\anaconda3\\lib\\site-packages (from selenium) (0.23.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: outcome in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: idna in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: selenium in c:\\anaconda3\\lib\\site-packages (4.15.1)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.15.2-py3-none-any.whl (10.2 MB)\n",
      "     ---------------------------------------- 10.2/10.2 MB 9.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\anaconda3\\lib\\site-packages (from selenium) (0.23.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: idna in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Installing collected packages: selenium\n",
      "  Attempting uninstall: selenium\n",
      "    Found existing installation: selenium 4.15.1\n",
      "    Uninstalling selenium-4.15.1:\n",
      "      Successfully uninstalled selenium-4.15.1\n",
      "Successfully installed selenium-4.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium beautifulsoup4\n",
    "!pip install --upgrade selenium\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b513fec2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WebDriver.__init__() got an unexpected keyword argument 'executable_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     57\u001b[0m     search_keywords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfruits\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcars\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMachine Learning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGuitar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCakes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 58\u001b[0m     \u001b[43mscrape_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_keywords\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m, in \u001b[0;36mscrape_images\u001b[1;34m(search_keywords, num_images)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape_images\u001b[39m(search_keywords, num_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m     driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath/to/chromedriver\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m search_keywords:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: WebDriver.__init__() got an unexpected keyword argument 'executable_path'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def scrape_images(search_keywords, num_images=10):\n",
    "    driver = webdriver.Chrome(executable_path='path/to/chromedriver')  \n",
    "\n",
    "    for keyword in search_keywords:\n",
    "        try:\n",
    "            search_images(driver, keyword, num_images)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while searching for '{keyword}': {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "def search_images(driver, keyword, num_images):\n",
    "    driver.get(\"https://images.google.com/\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    search_box = driver.find_element(\"name\", \"q\")\n",
    "    search_box.clear()\n",
    "    search_box.send_keys(keyword)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(2)\n",
    "\n",
    "    for _ in range(3):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    img_tags = soup.find_all('img', class_='rg_i')\n",
    "\n",
    "    image_urls = []\n",
    "    for img_tag in img_tags[:num_images]:\n",
    "        img_url = img_tag.get('data-src')\n",
    "        if img_url:\n",
    "            image_urls.append(img_url)\n",
    "\n",
    "    download_images(keyword, image_urls)\n",
    "\n",
    "def download_images(keyword, image_urls):\n",
    "    save_folder = f\"{keyword}_images\"\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "    for i, img_url in enumerate(image_urls):\n",
    "        try:\n",
    "            response = requests.get(img_url)\n",
    "            with open(os.path.join(save_folder, f\"{keyword}_{i+1}.jpg\"), 'wb') as img_file:\n",
    "                img_file.write(response.content)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while downloading image {i+1}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    scrape_images(search_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70d5f43",
   "metadata": {},
   "source": [
    "# Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eee1f064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\anaconda3\\lib\\site-packages (4.15.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pandas in c:\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\anaconda3\\lib\\site-packages (from selenium) (0.23.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: idna in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: outcome in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "pip install selenium pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9a0470f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 65\u001b[0m     product_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter the smartphone you want to search for on Flipkart: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     scrape_flipkart_smartphones(product_name)\n",
      "File \u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1175\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1174\u001b[0m     )\n\u001b[1;32m-> 1175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py:1217\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1216\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_smartphones(product_name):\n",
    "    driver = webdriver.Chrome(executable_path='path/to/chromedriver') \n",
    "    driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "    close_popup_button = driver.find_element(By.CSS_SELECTOR, \"button._2KpZ6l._2doB4z\")\n",
    "    if close_popup_button.is_displayed():\n",
    "        close_popup_button.click()\n",
    "\n",
    "    search_box = driver.find_element(By.NAME, \"q\")\n",
    "    search_box.clear()\n",
    "    search_box.send_keys(product_name)\n",
    "    search_box.submit()\n",
    "\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    details_list = []\n",
    "    product_cards = driver.find_elements(By.CSS_SELECTOR, \"div._1AtVbE\")\n",
    "\n",
    "    for card in product_cards:\n",
    "        details = extract_product_details(card)\n",
    "        details_list.append(details)\n",
    "\n",
    "    columns = [\"Brand Name\", \"Smartphone Name\", \"Colour\", \"RAM\", \"Storage (ROM)\", \n",
    "               \"Primary Camera\", \"Secondary Camera\", \"Display Size\", \n",
    "               \"Battery Capacity\", \"Price\", \"Product URL\"]\n",
    "    df = pd.DataFrame(details_list, columns=columns)\n",
    "\n",
    "    df.to_csv(\"flipkart_smartphones.csv\", index=False)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "def extract_product_details(card):\n",
    "    details = {}\n",
    "    details[\"Brand Name\"] = card.find_element(By.CSS_SELECTOR, \"div._2WkVRV\").text\n",
    "    details[\"Smartphone Name\"] = card.find_element(By.CSS_SELECTOR, \"a._1fQZEK\").text\n",
    "    details[\"Colour\"] = get_text(card, \"div._2o7WAb > ul > li:nth-child(4)\")\n",
    "    details[\"RAM\"] = get_text(card, \"div._2o7WAb > ul > li:nth-child(1)\")\n",
    "    details[\"Storage (ROM)\"] = get_text(card, \"div._2o7WAb > ul > li:nth-child(2)\")\n",
    "    details[\"Primary Camera\"] = get_text(card, \"div._2o7WAb > ul > li:nth-child(3)\")\n",
    "    details[\"Secondary Camera\"] = get_text(card, \"div._2o7WAb > ul > li:nth-child(5)\")\n",
    "    details[\"Display Size\"] = get_text(card, \"div._2o7WAb > ul > li:nth-child(6)\")\n",
    "    details[\"Battery Capacity\"] = get_text(card, \"div._2o7WAb > ul > li:nth-child(7)\")\n",
    "    details[\"Price\"] = card.find_element(By.CSS_SELECTOR, \"div._30jeq3\").text\n",
    "    details[\"Product URL\"] = card.find_element(By.CSS_SELECTOR, \"a._1fQZEK\").get_attribute(\"href\")\n",
    "\n",
    "    return details\n",
    "\n",
    "def get_text(element, selector):\n",
    "    try:\n",
    "        return element.find_element(By.CSS_SELECTOR, selector).text\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    product_name = input(\"Enter the smartphone you want to search for on Flipkart: \")\n",
    "    scrape_flipkart_smartphones(product_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a294de1f",
   "metadata": {},
   "source": [
    "# 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d85c654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in c:\\anaconda3\\lib\\site-packages (2.4.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\anaconda3\\lib\\site-packages (from geopy) (2.0)\n"
     ]
    }
   ],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9244b4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location(Shantinagar, Bangalor, Karnatak, (12.95756, 77.59693, 0.0))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from geopy.geocoders import ArcGIS\n",
    "nom = ArcGIS()\n",
    "nom.geocode('shantinagar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "72bf0202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location(Modinagar, Ghaziabad, Uttar Pradesh, (28.83624, 77.57679, 0.0))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nom.geocode('modinagar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9f3b7a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location(Delhi, Connaught Place, New Delhi, Delhi, (28.63409, 77.21693, 0.0))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nom.geocode('delhi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db198b0",
   "metadata": {},
   "source": [
    "# 6. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8fb40ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Processor, RAM, OS, Storage, Display, Price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_digit_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        laptops = []\n",
    "\n",
    "        laptop_cards = soup.find_all(\"div\", class_=\"TopNumbeHeading\")\n",
    "\n",
    "        for card in laptop_cards:\n",
    "            details = get_laptop_details(card)\n",
    "            laptops.append(details)\n",
    "\n",
    "        columns = [\"Name\", \"Processor\", \"RAM\", \"OS\", \"Storage\", \"Display\", \"Price\"]\n",
    "        df = pd.DataFrame(laptops, columns=columns)\n",
    "\n",
    "        print(df)\n",
    "        df.to_csv(\"digit_gaming_laptops.csv\", index=False)\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
    "\n",
    "def get_laptop_details(card):\n",
    "    details = {}\n",
    "    details[\"Name\"] = card.find(\"div\", class_=\"TopNumbeHeading\").text.strip()\n",
    "    details[\"Processor\"] = card.find(\"div\", class_=\"prod-spec\").text.strip()\n",
    "    details[\"RAM\"] = card.find(\"div\", class_=\"Spcs-details\").find_all(\"td\")[1].text.strip()\n",
    "    details[\"OS\"] = card.find(\"div\", class_=\"Spcs-details\").find_all(\"td\")[3].text.strip()\n",
    "    details[\"Storage\"] = card.find(\"div\", class_=\"Spcs-details\").find_all(\"td\")[5].text.strip()\n",
    "    details[\"Display\"] = card.find(\"div\", class_=\"Spcs-details\").find_all(\"td\")[7].text.strip()\n",
    "    details[\"Price\"] = card.find(\"div\", class_=\"TopPriceBox\").text.strip()\n",
    "\n",
    "    return details\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_digit_gaming_laptops()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d624bce0",
   "metadata": {},
   "source": [
    "# Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "86711eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table not found on the page.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        table = soup.find(\"table\", class_=\"data\")\n",
    "\n",
    "        if table:\n",
    "            billionaires = []\n",
    "\n",
    "            rows = table.find_all(\"tr\")[1:] \n",
    "\n",
    "            for row in rows:\n",
    "                details = get_billionaire_details(row)\n",
    "                billionaires.append(details)\n",
    "\n",
    "            columns = [\"Rank\", \"Name\", \"Net Worth\", \"Age\", \"Citizenship\", \"Source\", \"Industry\"]\n",
    "            df = pd.DataFrame(billionaires, columns=columns)\n",
    "\n",
    "            print(df)\n",
    "            df.to_csv(\"forbes_billionaires.csv\", index=False)\n",
    "\n",
    "        else:\n",
    "            print(\"Table not found on the page.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
    "\n",
    "def get_billionaire_details(row):\n",
    "    details = {}\n",
    "    columns = row.find_all(\"td\")\n",
    "\n",
    "    details[\"Rank\"] = columns[0].text.strip()\n",
    "    details[\"Name\"] = columns[1].text.strip()\n",
    "    details[\"Net Worth\"] = columns[2].text.strip()\n",
    "    details[\"Age\"] = columns[3].text.strip()\n",
    "    details[\"Citizenship\"] = columns[4].text.strip()\n",
    "    details[\"Source\"] = columns[5].text.strip()\n",
    "    details[\"Industry\"] = columns[6].text.strip()\n",
    "\n",
    "    return details\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_forbes_billionaires()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c93aa9d",
   "metadata": {},
   "source": [
    "# 8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7476989c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'capabilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:38\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[1;34m(service, options)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mSeleniumManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\selenium_manager.py:75\u001b[0m, in \u001b[0;36mSeleniumManager.driver_location\u001b[1;34m(self, options)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"Determines the path of the correct driver.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m:Args:\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m - browser: which browser to get the driver path for.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m:Returns: The driver path to use\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m browser \u001b[38;5;241m=\u001b[39m \u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapabilities\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     77\u001b[0m args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_binary()), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--browser\u001b[39m\u001b[38;5;124m\"\u001b[39m, browser]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'capabilities'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Set up the WebDriver\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_to_chromedriver\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace with the path to your WebDriver executable\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Open the YouTube video\u001b[39;00m\n\u001b[0;32m      8\u001b[0m video_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.youtube.com/watch?v=your_video_id\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with the URL of the YouTube video\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDesiredCapabilities\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHROME\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbrowserName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:51\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvendor_prefix \u001b[38;5;241m=\u001b[39m vendor_prefix\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m \u001b[43mDriverFinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:40\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[1;34m(service, options)\u001b[0m\n\u001b[0;32m     38\u001b[0m     path \u001b[38;5;241m=\u001b[39m SeleniumManager()\u001b[38;5;241m.\u001b[39mdriver_location(options) \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m---> 40\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;241m.\u001b[39mcapabilities[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using Selenium Manager.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mis_file():\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'capabilities'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# Set up the WebDriver\n",
    "driver = webdriver.Chrome('path_to_chromedriver')  # Replace with the path to your WebDriver executable\n",
    "\n",
    "# Open the YouTube video\n",
    "video_url = 'https://www.youtube.com/watch?v=your_video_id'  # Replace with the URL of the YouTube video\n",
    "driver.get(video_url)\n",
    "\n",
    "\n",
    "scroll_pause_time = 2  \n",
    "scrolls = 10  \n",
    "\n",
    "for _ in range(scrolls):\n",
    "  driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "  time.sleep(scroll_pause_time)\n",
    "\n",
    "comments = driver.find_elements_by_xpath('//yt-formatted-string[@id=\"content-text\"]')\n",
    "upvotes = driver.find_elements_by_xpath('//span[@id=\"vote-count-middle\"]')\n",
    "times = driver.find_elements_by_xpath('//a[@class=\"yt-simple-endpoint style-scope yt-formatted-string\"]')\n",
    "\n",
    "extracted_data = []\n",
    "for comment, upvote, time in zip(comments, upvotes, times):\n",
    "  extracted_data.append({\n",
    "  'comment': comment.text,\n",
    "  'upvote': upvote.text,\n",
    "  'time': time.text\n",
    "  })\n",
    "\n",
    "driver.quit()\n",
    "for data in extracted_data:\n",
    "  print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242a911",
   "metadata": {},
   "source": [
    "# Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "693bc7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.hostelworld.com/hostels/London\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "hostels = soup.find_all(\"div\", class_=\"fabresult\")\n",
    "\n",
    "for hostel in hostels:\n",
    "    name = hostel.find(\"h2\", class_=\"fabresult-title\").text.strip()\n",
    "    distance = hostel.find(\"span\", class_=\"distance\").text.strip()\n",
    "    ratings = hostel.find(\"div\", class_=\"rating\").text.strip()\n",
    "  \n",
    "    total_reviews = hostel.find(\"div\", class_=\"reviews\").text.strip()\n",
    "    overall_reviews = hostel.find(\"div\", class_=\"overall\").text.strip()\n",
    "    privates_price = hostel.find(\"div\", class_=\"price-col\").find(\"div\", class_=\"price\").text.strip()\n",
    "    dorms_price = hostel.find(\"div\", class_=\"price-col\").find(\"div\", class_=\"price\").find_next_sibling(\"div\", class_=\"price\").text.strip()\n",
    "    facilities = hostel.find(\"div\", class_=\"facilities\").text.strip()\n",
    "    description = hostel.find(\"div\", class_=\"description\").text.strip()\n",
    "  \n",
    "  \n",
    "    print(\"Hostel Name:\", name)\n",
    "    print(\"Distance from City Centre:\", distance)\n",
    "    print(\"Ratings:\", ratings)\n",
    "    print(\"Total Reviews:\", total_reviews)\n",
    "    print(\"Overall Reviews:\", overall_reviews)\n",
    "    print(\"Privates from Price:\", privates_price)\n",
    "    print(\"Dorms from Price:\", dorms_price)\n",
    "    print(\"Facilities:\", facilities)\n",
    "    print(\"Property Description:\", description)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe0d96f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
